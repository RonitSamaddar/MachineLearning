{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport keras\nfrom keras.datasets import fashion_mnist#download mnist data and split into train and test sets\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout#create model\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\n(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=10000, random_state=42)","execution_count":2,"outputs":[{"output_type":"stream","text":"Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n32768/29515 [=================================] - 0s 3us/step\nDownloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n26427392/26421880 [==============================] - 2s 0us/step\nDownloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n8192/5148 [===============================================] - 0s 0us/step\nDownloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n4423680/4422102 [==============================] - 1s 0us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Seeing dimensions of the different sets\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_val.shape)\nprint(y_val.shape)\nprint(X_test.shape)\nprint(y_test.shape)","execution_count":3,"outputs":[{"output_type":"stream","text":"(50000, 28, 28)\n(50000,)\n(10000, 28, 28)\n(10000,)\n(10000, 28, 28)\n(10000,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Seeing example of random images\nimage_no=5\nimg=X_train[image_no]\nplt.imshow(img)","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f17321f3190>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_X(X):\n    X_norm=X/255\n    return X_norm","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reshape_X(X):\n    X_reshaped=X.reshape(X.shape[0],28,28,1)\n    return X_reshaped","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def input_and_run(model,X_train,X_val,X_test,y_train,y_val,y_test,alpha=0.01,num_epochs=10):\n    #Pre processing the data\n    X_train=reshape_X(normalize_X(X_train))\n    X_val=reshape_X(normalize_X(X_val))\n    X_test=reshape_X(normalize_X(X_test))\n    y_train=to_categorical(y_train)\n    y_val=to_categorical(y_val)\n    y_test=to_categorical(y_test)\n    \n    #compile model using accuracy to measure model performance\n    opt = keras.optimizers.Adam(learning_rate=alpha)\n    model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n    \n    #train the model\n    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=num_epochs)\n    \n    #Getting results\n    result = model.evaluate(X_train,y_train)\n    #print(result)\n    print(\"Training accuracy = \"+str(result[1]*100))\n    result = model.evaluate(X_val,y_val)\n    #print(result)\n    print(\"Validation accuracy = \"+str(result[1]*100))\n    result = model.evaluate(X_test,y_test)\n    #print(result)\n    print(\"Test accuracy = \"+str(result[1]*100))\n","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##BUILDING THE MODEL 1\n\nmodel1 = Sequential()#add model layers\n\nmodel1.add(Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(28,28,1)))\nmodel1.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\nmodel1.add(Flatten())\nmodel1.add(Dense(10, activation='softmax'))\n\nprint(model1.summary())\ninput_and_run(model1,X_train,X_val,X_test,y_train,y_val,y_test,alpha=0.01,num_epochs=20)","execution_count":8,"outputs":[{"output_type":"stream","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 26, 26, 64)        640       \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 24, 24, 32)        18464     \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 18432)             0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                184330    \n=================================================================\nTotal params: 203,434\nTrainable params: 203,434\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 50000 samples, validate on 10000 samples\nEpoch 1/20\n50000/50000 [==============================] - 11s 223us/step - loss: 0.4568 - accuracy: 0.8396 - val_loss: 0.4178 - val_accuracy: 0.8532\nEpoch 2/20\n50000/50000 [==============================] - 7s 141us/step - loss: 0.3360 - accuracy: 0.8806 - val_loss: 0.3472 - val_accuracy: 0.8778\nEpoch 3/20\n50000/50000 [==============================] - 7s 136us/step - loss: 0.3013 - accuracy: 0.8918 - val_loss: 0.3570 - val_accuracy: 0.8758\nEpoch 4/20\n50000/50000 [==============================] - 8s 157us/step - loss: 0.2755 - accuracy: 0.8995 - val_loss: 0.4014 - val_accuracy: 0.8694\nEpoch 5/20\n50000/50000 [==============================] - 7s 144us/step - loss: 0.2620 - accuracy: 0.9049 - val_loss: 0.3948 - val_accuracy: 0.8661\nEpoch 6/20\n50000/50000 [==============================] - 7s 136us/step - loss: 0.2487 - accuracy: 0.9091 - val_loss: 0.3732 - val_accuracy: 0.8817\nEpoch 7/20\n50000/50000 [==============================] - 7s 139us/step - loss: 0.2356 - accuracy: 0.9138 - val_loss: 0.3814 - val_accuracy: 0.8824\nEpoch 8/20\n50000/50000 [==============================] - 7s 138us/step - loss: 0.2247 - accuracy: 0.9161 - val_loss: 0.4119 - val_accuracy: 0.8737\nEpoch 9/20\n50000/50000 [==============================] - 7s 140us/step - loss: 0.2232 - accuracy: 0.9184 - val_loss: 0.4435 - val_accuracy: 0.8671\nEpoch 10/20\n50000/50000 [==============================] - 7s 142us/step - loss: 0.2233 - accuracy: 0.9174 - val_loss: 0.4360 - val_accuracy: 0.8780\nEpoch 11/20\n50000/50000 [==============================] - 7s 135us/step - loss: 0.2115 - accuracy: 0.9229 - val_loss: 0.4391 - val_accuracy: 0.8778\nEpoch 12/20\n50000/50000 [==============================] - 7s 145us/step - loss: 0.2060 - accuracy: 0.9226 - val_loss: 0.4630 - val_accuracy: 0.8765\nEpoch 13/20\n50000/50000 [==============================] - 8s 152us/step - loss: 0.2021 - accuracy: 0.9241 - val_loss: 0.4799 - val_accuracy: 0.8584\nEpoch 14/20\n50000/50000 [==============================] - 7s 137us/step - loss: 0.1999 - accuracy: 0.9261 - val_loss: 0.5249 - val_accuracy: 0.8666\nEpoch 15/20\n50000/50000 [==============================] - 7s 138us/step - loss: 0.1934 - accuracy: 0.9280 - val_loss: 0.4364 - val_accuracy: 0.8722\nEpoch 16/20\n50000/50000 [==============================] - 7s 137us/step - loss: 0.1915 - accuracy: 0.9287 - val_loss: 0.4817 - val_accuracy: 0.8730\nEpoch 17/20\n50000/50000 [==============================] - 7s 137us/step - loss: 0.1930 - accuracy: 0.9275 - val_loss: 0.4967 - val_accuracy: 0.8676\nEpoch 18/20\n50000/50000 [==============================] - 7s 139us/step - loss: 0.1901 - accuracy: 0.9291 - val_loss: 0.5095 - val_accuracy: 0.8672\nEpoch 19/20\n50000/50000 [==============================] - 7s 140us/step - loss: 0.1848 - accuracy: 0.9317 - val_loss: 0.5192 - val_accuracy: 0.8700\nEpoch 20/20\n50000/50000 [==============================] - 7s 139us/step - loss: 0.1848 - accuracy: 0.9317 - val_loss: 0.6013 - val_accuracy: 0.8716\n50000/50000 [==============================] - 4s 76us/step\nTraining accuracy = 94.18799877166748\n10000/10000 [==============================] - 1s 71us/step\nValidation accuracy = 87.15999722480774\n10000/10000 [==============================] - 1s 86us/step\nTest accuracy = 86.9599997997284\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"##BUILDING THE MODEL 2\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten#create model\n\nmodel2 = Sequential()#add model layers\n\nmodel2.add(Conv2D(32, kernel_size=3, activation=\"relu\", input_shape=(28,28,1)))\nmodel2.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\nmodel2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\nmodel2.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\nmodel2.add(Conv2D(128, kernel_size=3, activation=\"relu\"))\nmodel2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\nmodel2.add(Flatten())\nmodel2.add(Dense(10, activation='softmax'))\n\nprint(model2.summary())\ninput_and_run(model2,X_train,X_val,X_test,y_train,y_val,y_test,alpha=0.01,num_epochs=10)","execution_count":9,"outputs":[{"output_type":"stream","text":"Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 24, 24, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 10, 10, 64)        36928     \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 8, 8, 128)         73856     \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 2048)              0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 10)                20490     \n=================================================================\nTotal params: 150,090\nTrainable params: 150,090\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 50000 samples, validate on 10000 samples\nEpoch 1/10\n50000/50000 [==============================] - 9s 187us/step - loss: 0.5222 - accuracy: 0.8074 - val_loss: 0.4080 - val_accuracy: 0.8485\nEpoch 2/10\n50000/50000 [==============================] - 8s 168us/step - loss: 0.4013 - accuracy: 0.8543 - val_loss: 0.3705 - val_accuracy: 0.8623\nEpoch 3/10\n50000/50000 [==============================] - 8s 166us/step - loss: 0.3885 - accuracy: 0.8566 - val_loss: 0.4312 - val_accuracy: 0.8421\nEpoch 4/10\n50000/50000 [==============================] - 8s 166us/step - loss: 0.3774 - accuracy: 0.8613 - val_loss: 0.3767 - val_accuracy: 0.8576\nEpoch 5/10\n50000/50000 [==============================] - 9s 170us/step - loss: 0.3739 - accuracy: 0.8615 - val_loss: 0.3913 - val_accuracy: 0.8548\nEpoch 6/10\n50000/50000 [==============================] - 8s 166us/step - loss: 0.3714 - accuracy: 0.8630 - val_loss: 0.3769 - val_accuracy: 0.8621\nEpoch 7/10\n50000/50000 [==============================] - 9s 172us/step - loss: 0.3668 - accuracy: 0.8643 - val_loss: 0.3976 - val_accuracy: 0.8566\nEpoch 8/10\n50000/50000 [==============================] - 9s 175us/step - loss: 0.3670 - accuracy: 0.8624 - val_loss: 0.3928 - val_accuracy: 0.8568\nEpoch 9/10\n50000/50000 [==============================] - 8s 170us/step - loss: 0.3712 - accuracy: 0.8634 - val_loss: 0.4485 - val_accuracy: 0.8378\nEpoch 10/10\n50000/50000 [==============================] - 8s 168us/step - loss: 0.3660 - accuracy: 0.8641 - val_loss: 0.4030 - val_accuracy: 0.8511\n50000/50000 [==============================] - 4s 73us/step\nTraining accuracy = 86.21600270271301\n10000/10000 [==============================] - 1s 73us/step\nValidation accuracy = 85.11000275611877\n10000/10000 [==============================] - 1s 73us/step\nTest accuracy = 85.10000109672546\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# The four layer model seems to be not up to the mark.\n# Lets try another four layer model (WITHOUT POOL) before getting back to single layer modifications"},{"metadata":{"trusted":true},"cell_type":"code","source":"##BUILDING THE MODEL 3\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten#create model\n\nmodel3 = Sequential()#add model layers\n\nmodel3.add(Conv2D(32, kernel_size=3, activation=\"relu\", input_shape=(28,28,1)))\nmodel3.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\n\n\nmodel3.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\nmodel3.add(Conv2D(128, kernel_size=3, activation=\"relu\"))\n\n\nmodel3.add(Flatten())\nmodel3.add(Dense(10, activation='softmax'))\n\nprint(model3.summary())\ninput_and_run(model3,X_train,X_val,X_test,y_train,y_val,y_test,alpha=0.01,num_epochs=10)","execution_count":null,"outputs":[{"output_type":"stream","text":"Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_7 (Conv2D)            (None, 26, 26, 32)        320       \n_________________________________________________________________\nconv2d_8 (Conv2D)            (None, 24, 24, 64)        18496     \n_________________________________________________________________\nconv2d_9 (Conv2D)            (None, 22, 22, 64)        36928     \n_________________________________________________________________\nconv2d_10 (Conv2D)           (None, 20, 20, 128)       73856     \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 51200)             0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 10)                512010    \n=================================================================\nTotal params: 641,610\nTrainable params: 641,610\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 50000 samples, validate on 10000 samples\nEpoch 1/10\n50000/50000 [==============================] - 10s 209us/step - loss: 0.5534 - accuracy: 0.8147 - val_loss: 0.4215 - val_accuracy: 0.8469\nEpoch 2/10\n50000/50000 [==============================] - 10s 205us/step - loss: 0.4187 - accuracy: 0.8507 - val_loss: 0.4656 - val_accuracy: 0.8327\nEpoch 3/10\n18528/50000 [==========>...................] - ETA: 5s - loss: 0.3696 - accuracy: 0.8670","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"\n# This four layer model still seems to be not up to the mark.\n# Lets try two layer modifications"},{"metadata":{"trusted":true},"cell_type":"code","source":"##BUILDING THE MODEL 4\n\nmodel4 = Sequential()#add model layers\n\nmodel4.add(Conv2D(32, kernel_size=3, activation=\"relu\", input_shape=(28,28,1)))\n#model4.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))#Having this changed accuracy from 92 to 89\nmodel4.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\n#model4.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))#Removing this changed accuracy from 92 to 93\nmodel4.add(Flatten())\nmodel4.add(Dense(10, activation='softmax'))\n\nprint(model4.summary())\ninput_and_run(model4,X_train,X_val,X_test,y_train,y_val,y_test,alpha=0.01,num_epochs=20)\n\n## Train Accuracy = 94.79 or 96.75 or 95.32 or 95.25 or 95.39\n## Val Accuracy =                              87.48 or 85.79\n## Test Accuracy =                             87.11 or 85.18","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}